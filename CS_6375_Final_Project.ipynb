{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NfDc2s-UDH-"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "class TicTacToe:\n",
        "    def __init__(self):\n",
        "        self.gameBoard = np.zeros((3, 3))\n",
        "        # 1 for agent 1, -1 for agent 2\n",
        "        self.player = 1\n",
        "        self.gameWinner = None\n",
        "        self.isGameDone = False\n",
        "\n",
        "    # reset game board to the beginning with no plays\n",
        "    def reset_game_board(self):\n",
        "      self.gameBoard = np.zeros((3, 3))\n",
        "      self.player, self.gameWinner, self.isGameDone = 1, None, False\n",
        "\n",
        "    # flatten the game board into 1D array\n",
        "    def get_flattened_state(self):\n",
        "      flattened_board = self.gameBoard.flatten()\n",
        "      state_tuple = tuple(flattened_board)\n",
        "      return state_tuple\n",
        "\n",
        "    # check if the move is a valid move\n",
        "    def is_valid_move(self, action):\n",
        "      if 0 <= action < 9:\n",
        "          if self.gameBoard.flatten()[action] == 0:\n",
        "              return True\n",
        "          else:\n",
        "              return False\n",
        "      else:\n",
        "          return False\n",
        "\n",
        "    # update the board with the given move\n",
        "    def make_board_move(self, action):\n",
        "        if not self.is_valid_move(action):\n",
        "            return False\n",
        "\n",
        "        self._apply_move(action)\n",
        "        self.check_game_winner() # after applying the move check if there is a winner\n",
        "        self.player *= -1\n",
        "\n",
        "        # if there is a winner or the game board is full, the game is over\n",
        "        if np.count_nonzero(self.gameBoard == 0) == 0 or self.gameWinner is not None:\n",
        "            self.isGameDone = True\n",
        "\n",
        "        return True\n",
        "\n",
        "    # apply the given move if it is valid\n",
        "    def _apply_move(self, action):\n",
        "      row, col = divmod(action, 3)\n",
        "      self.gameBoard[row, col] = self.player\n",
        "\n",
        "    # check if the game has been won\n",
        "    def check_game_winner(self):\n",
        "      # Check rows\n",
        "      for row in self.gameBoard:\n",
        "          if all(cell == self.player for cell in row):\n",
        "              self.gameWinner = self.player\n",
        "              return\n",
        "\n",
        "      # Check columns\n",
        "      for col in range(3):\n",
        "          if all(self.gameBoard[row, col] == self.player for row in range(3)):\n",
        "              self.gameWinner = self.player\n",
        "              return\n",
        "\n",
        "      # Check diagonals\n",
        "      if all(self.gameBoard[i, i] == self.player for i in range(3)) or \\\n",
        "        all(self.gameBoard[i, 2 - i] == self.player for i in range(3)):\n",
        "          self.gameWinner = self.player\n",
        "\n",
        "    # display the current game state\n",
        "    def display_board(self):\n",
        "      border = \"-------------\"\n",
        "\n",
        "      for i, row in enumerate(self.gameBoard):\n",
        "          print(\" \" + \" | \".join([\"X\" if cell == 1 else \"O\" if cell == -1 else \" \" for cell in row]))\n",
        "          if i < 2:\n",
        "              print(border)\n",
        "\n",
        "      print()\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, player, ee_threshold=0.1, learn_rate=0.1, decay_rate=0.9):\n",
        "        self.qtable = {}\n",
        "        self.ee_threshold = ee_threshold # Explore-exploit threshold\n",
        "        self.learn_rate = learn_rate  # Learning rate\n",
        "        self.decay_rate = decay_rate  # Decay rate\n",
        "        self.player = player  # current player\n",
        "\n",
        "    # return the current q value\n",
        "    def return_q_value(self, state, action):\n",
        "        q_value = self.qtable.get((state, action), 0.0)\n",
        "        return q_value\n",
        "\n",
        "    # agent selects the next action\n",
        "    def pick_next_action(self, state, available_moves):\n",
        "        if np.random.rand() < self.ee_threshold:\n",
        "            return np.random.choice(available_moves)\n",
        "        else:\n",
        "            q_values = [self.return_q_value(state, action) for action in available_moves]\n",
        "            return available_moves[np.argmax(q_values)]\n",
        "\n",
        "    # update the q value for the given action, reward amount, and next state of the board\n",
        "    def update_q_value(self, state, action, reward, next_state):\n",
        "        if reward == 0:  # Update the reward for a tie - reward for tie is 0.8\n",
        "            reward = 0.8\n",
        "\n",
        "        # agent picks the next best move on the current board\n",
        "        best_move = max(self.return_q_value(next_state, next_move) for next_move in range(9))\n",
        "        current_qvalue = self.return_q_value(state, action)\n",
        "        # calculate the new qvalue for the new action\n",
        "        new_qvalue = current_qvalue + self.learn_rate * (reward + self.decay_rate * best_move - current_qvalue)\n",
        "        self.qtable[(state, action)] = new_qvalue\n",
        "\n",
        "class RandomAgent:\n",
        "    def __init__(self, player):\n",
        "        self.player = player\n",
        "\n",
        "    # for random player agent, just pick a random move from the available possible moves\n",
        "    def pick_next_action(self, state, available_moves):\n",
        "        return random.choice(available_moves)\n",
        "\n",
        "def train_agent(agent, episodes=10000):\n",
        "    env = TicTacToe()\n",
        "    gameWin = 0\n",
        "    gameTie = 0\n",
        "    gameLost = 0\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        env.reset_game_board()\n",
        "        state = env.get_flattened_state()\n",
        "\n",
        "        while not env.isGameDone:\n",
        "            # get the possible move choices\n",
        "            available_moves = get_available_moves(env)\n",
        "            action = agent.pick_next_action(state, available_moves)\n",
        "\n",
        "            # make the next move\n",
        "            env.make_board_move(action)\n",
        "            next_state = env.get_flattened_state()\n",
        "\n",
        "            if env.gameWinner == 1:\n",
        "                reward = 1\n",
        "            elif env.gameWinner == -1:\n",
        "                reward = -1\n",
        "            else:\n",
        "                reward = 0.8\n",
        "\n",
        "            # update the q table with the current action, reward, and next state\n",
        "            agent.update_q_value(state, action, reward, next_state)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        if env.gameWinner == 1:\n",
        "            gameWin += 1\n",
        "        if env.gameWinner == -1:\n",
        "            gameLost += 1\n",
        "        if env.gameWinner == 0:\n",
        "            gameTie += 1\n",
        "\n",
        "    display_training_results(gameWin, gameTie, gameLost)\n",
        "\n",
        "\n",
        "def display_training_results(gameWin, gameTie, gameLost):\n",
        "    print(\"=== Training Results ===\")\n",
        "    print(f\"Total Wins: {gameWin}\")\n",
        "    print(f\"Total Draws: {gameTie}\")\n",
        "    print(f\"Total Losses: {gameLost}\")\n",
        "    print(\"=======================\")\n",
        "\n",
        "# test two agents playing against each other\n",
        "def run_agents(agent1, agent2, episodes=100):\n",
        "    player1_wins = 0\n",
        "    player2_wins = 0\n",
        "    ties = 0\n",
        "\n",
        "    for _ in range(episodes):\n",
        "        env = TicTacToe()\n",
        "        state = env.get_flattened_state()\n",
        "\n",
        "        # while the game is not over, play the two agents\n",
        "        while not env.isGameDone:\n",
        "            if env.player == 1:\n",
        "                current_player = agent1\n",
        "            else:\n",
        "                current_player = agent2\n",
        "\n",
        "            available_moves = get_available_moves(env)\n",
        "            action = current_player.pick_next_action(state, available_moves)\n",
        "\n",
        "            env.make_board_move(action)\n",
        "            state = env.get_flattened_state()\n",
        "\n",
        "        if env.gameWinner == 1:\n",
        "            player1_wins += 1\n",
        "        elif env.gameWinner == -1:\n",
        "            player2_wins += 1\n",
        "        else:\n",
        "            ties += 1\n",
        "\n",
        "    display_results(player1_wins, player2_wins, ties, episodes)\n",
        "\n",
        "def get_available_moves(env):\n",
        "    return [i for i in range(9) if env.is_valid_move(i)]\n",
        "\n",
        "\n",
        "def display_results(player1_wins, player2_wins, ties, episodes):\n",
        "    print(\"Results:\")\n",
        "    print(f\"Agent 1 wins: {player1_wins}, Agent 1 win %: {player1_wins/episodes}\")\n",
        "    print(f\"Agent 2 wins: {player2_wins}, Agent 2 win %: {player2_wins/episodes}\")\n",
        "    print(f\"Ties: {ties}, Tie %: {ties/episodes}\")\n",
        "\n",
        "\n",
        "def compete_with_agent(agent):\n",
        "    env = TicTacToe()\n",
        "\n",
        "    while True:\n",
        "        # start a new game\n",
        "        env.reset_game_board()\n",
        "        state = env.get_flattened_state()\n",
        "\n",
        "        # show the current game state\n",
        "        while not env.isGameDone:\n",
        "            env.display_board()\n",
        "\n",
        "            if env.player == 1:  # player = 1 is the user\n",
        "               print(\"User's (X) turn. Pick any number 0 through 8:\")\n",
        "               while True:\n",
        "                 user_input = input()\n",
        "                 try:\n",
        "                     user_action = int(user_input)\n",
        "                     if not env.is_valid_move(user_action):\n",
        "                         print(\"Invalid input. Pick any number 0 through 8.\")\n",
        "                     else:\n",
        "                         env.make_board_move(user_action)\n",
        "                         break\n",
        "                 except ValueError:\n",
        "                     print(\"Invalid input. Pick any number 0 through 8.\")\n",
        "            else: # the q-learning agent's turn to play\n",
        "               available_moves = get_available_moves(env)\n",
        "               action = agent.pick_next_action(state, available_moves)\n",
        "               env.make_board_move(action)\n",
        "\n",
        "\n",
        "            state = env.get_flattened_state()\n",
        "\n",
        "          # display the end result of the game\n",
        "        env.display_board()\n",
        "        if env.gameWinner == 1:\n",
        "            print(\"User wins!\")\n",
        "        elif env.gameWinner == -1:\n",
        "             print(\"User lost!\")\n",
        "        else:\n",
        "            print(\"It's a tie!\")\n",
        "\n",
        "\n",
        "        print(\"Want to play another round? (Y/N)\")\n",
        "        choice = input().upper()\n",
        "        if choice != 'Y':\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    iterations = [1000, 10000]\n",
        "    learn_rate = [.1, .2]\n",
        "    decay_rate = [.8, .7]\n",
        "    explore_exploit = [0.2, 0.3]\n",
        "\n",
        "    for i in iterations:\n",
        "        for l in learn_rate:\n",
        "            for d in decay_rate:\n",
        "                for e in explore_exploit:\n",
        "                    print(f\"Iterations: {i}, Learn Rate: {l}, Decay Rate: {d}, Explore-Exploit Threshold: {e}\")\n",
        "                    # agent is trained and tested against an agent that makes random choices among available spots\n",
        "                    agent1 = QLearningAgent(player=1, ee_threshold=e, learn_rate=l, decay_rate=d)\n",
        "                    agent2 = RandomAgent(player=-1)\n",
        "\n",
        "                    train_agent(agent1, episodes=i)\n",
        "\n",
        "                    run_agents(agent1, agent2)\n",
        "                    print()\n",
        "\n",
        "    iterations = [1000, 2000, 5000, 10000, 15000, 20000]\n",
        "    learn_rate = [.1]\n",
        "    decay_rate = [.8]\n",
        "    explore_exploit = [0.2]\n",
        "\n",
        "    for i in iterations:\n",
        "        for l in learn_rate:\n",
        "            for d in decay_rate:\n",
        "                for e in explore_exploit:\n",
        "                    print(f\"Iterations: {i}, Learn Rate: {l}, Decay Rate: {d}, Explore-Exploit Threshold: {e}\")\n",
        "                    # agent is trained and tested against an agent that makes random choices among available spots\n",
        "                    agent1 = QLearningAgent(player=1, ee_threshold=e, learn_rate=l, decay_rate=d)\n",
        "                    agent2 = RandomAgent(player=-1)\n",
        "\n",
        "                    train_agent(agent1, episodes=i)\n",
        "\n",
        "                    run_agents(agent1, agent2)\n",
        "                    print()\n",
        "\n",
        "    # UNCOMMENT TO PLAY AGAINST THE Q-LEARNING AGENT\n",
        "    # compete_with_agent(agent1)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMOLSbeNobu4",
        "outputId": "5750e5aa-a85a-447f-b5e2-b43091052d5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations: 1000, Learn Rate: 0.1, Decay Rate: 0.8, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 761\n",
            "Total Draws: 0\n",
            "Total Losses: 70\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 75, Agent 1 win %: 0.75\n",
            "Agent 2 wins: 13, Agent 2 win %: 0.13\n",
            "Ties: 12, Tie %: 0.12\n",
            "\n",
            "Iterations: 1000, Learn Rate: 0.1, Decay Rate: 0.8, Explore-Exploit Threshold: 0.3\n",
            "=== Training Results ===\n",
            "Total Wins: 696\n",
            "Total Draws: 0\n",
            "Total Losses: 144\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 65, Agent 1 win %: 0.65\n",
            "Agent 2 wins: 26, Agent 2 win %: 0.26\n",
            "Ties: 9, Tie %: 0.09\n",
            "\n",
            "Iterations: 1000, Learn Rate: 0.1, Decay Rate: 0.7, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 743\n",
            "Total Draws: 0\n",
            "Total Losses: 98\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 74, Agent 1 win %: 0.74\n",
            "Agent 2 wins: 20, Agent 2 win %: 0.2\n",
            "Ties: 6, Tie %: 0.06\n",
            "\n",
            "Iterations: 1000, Learn Rate: 0.1, Decay Rate: 0.7, Explore-Exploit Threshold: 0.3\n",
            "=== Training Results ===\n",
            "Total Wins: 751\n",
            "Total Draws: 0\n",
            "Total Losses: 147\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 66, Agent 1 win %: 0.66\n",
            "Agent 2 wins: 20, Agent 2 win %: 0.2\n",
            "Ties: 14, Tie %: 0.14\n",
            "\n",
            "Iterations: 1000, Learn Rate: 0.2, Decay Rate: 0.8, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 807\n",
            "Total Draws: 0\n",
            "Total Losses: 70\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 82, Agent 1 win %: 0.82\n",
            "Agent 2 wins: 16, Agent 2 win %: 0.16\n",
            "Ties: 2, Tie %: 0.02\n",
            "\n",
            "Iterations: 1000, Learn Rate: 0.2, Decay Rate: 0.8, Explore-Exploit Threshold: 0.3\n",
            "=== Training Results ===\n",
            "Total Wins: 722\n",
            "Total Draws: 0\n",
            "Total Losses: 131\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 73, Agent 1 win %: 0.73\n",
            "Agent 2 wins: 18, Agent 2 win %: 0.18\n",
            "Ties: 9, Tie %: 0.09\n",
            "\n",
            "Iterations: 1000, Learn Rate: 0.2, Decay Rate: 0.7, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 790\n",
            "Total Draws: 0\n",
            "Total Losses: 70\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 61, Agent 1 win %: 0.61\n",
            "Agent 2 wins: 29, Agent 2 win %: 0.29\n",
            "Ties: 10, Tie %: 0.1\n",
            "\n",
            "Iterations: 1000, Learn Rate: 0.2, Decay Rate: 0.7, Explore-Exploit Threshold: 0.3\n",
            "=== Training Results ===\n",
            "Total Wins: 702\n",
            "Total Draws: 0\n",
            "Total Losses: 155\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 60, Agent 1 win %: 0.6\n",
            "Agent 2 wins: 29, Agent 2 win %: 0.29\n",
            "Ties: 11, Tie %: 0.11\n",
            "\n",
            "Iterations: 10000, Learn Rate: 0.1, Decay Rate: 0.8, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 7663\n",
            "Total Draws: 0\n",
            "Total Losses: 599\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 69, Agent 1 win %: 0.69\n",
            "Agent 2 wins: 26, Agent 2 win %: 0.26\n",
            "Ties: 5, Tie %: 0.05\n",
            "\n",
            "Iterations: 10000, Learn Rate: 0.1, Decay Rate: 0.8, Explore-Exploit Threshold: 0.3\n",
            "=== Training Results ===\n",
            "Total Wins: 7561\n",
            "Total Draws: 0\n",
            "Total Losses: 968\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 65, Agent 1 win %: 0.65\n",
            "Agent 2 wins: 18, Agent 2 win %: 0.18\n",
            "Ties: 17, Tie %: 0.17\n",
            "\n",
            "Iterations: 10000, Learn Rate: 0.1, Decay Rate: 0.7, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 8024\n",
            "Total Draws: 0\n",
            "Total Losses: 1131\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 66, Agent 1 win %: 0.66\n",
            "Agent 2 wins: 26, Agent 2 win %: 0.26\n",
            "Ties: 8, Tie %: 0.08\n",
            "\n",
            "Iterations: 10000, Learn Rate: 0.1, Decay Rate: 0.7, Explore-Exploit Threshold: 0.3\n",
            "=== Training Results ===\n",
            "Total Wins: 6600\n",
            "Total Draws: 0\n",
            "Total Losses: 1837\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 56, Agent 1 win %: 0.56\n",
            "Agent 2 wins: 30, Agent 2 win %: 0.3\n",
            "Ties: 14, Tie %: 0.14\n",
            "\n",
            "Iterations: 10000, Learn Rate: 0.2, Decay Rate: 0.8, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 7060\n",
            "Total Draws: 0\n",
            "Total Losses: 949\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 62, Agent 1 win %: 0.62\n",
            "Agent 2 wins: 27, Agent 2 win %: 0.27\n",
            "Ties: 11, Tie %: 0.11\n",
            "\n",
            "Iterations: 10000, Learn Rate: 0.2, Decay Rate: 0.8, Explore-Exploit Threshold: 0.3\n",
            "=== Training Results ===\n",
            "Total Wins: 7401\n",
            "Total Draws: 0\n",
            "Total Losses: 1117\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 63, Agent 1 win %: 0.63\n",
            "Agent 2 wins: 29, Agent 2 win %: 0.29\n",
            "Ties: 8, Tie %: 0.08\n",
            "\n",
            "Iterations: 10000, Learn Rate: 0.2, Decay Rate: 0.7, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 7678\n",
            "Total Draws: 0\n",
            "Total Losses: 573\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 65, Agent 1 win %: 0.65\n",
            "Agent 2 wins: 27, Agent 2 win %: 0.27\n",
            "Ties: 8, Tie %: 0.08\n",
            "\n",
            "Iterations: 10000, Learn Rate: 0.2, Decay Rate: 0.7, Explore-Exploit Threshold: 0.3\n",
            "=== Training Results ===\n",
            "Total Wins: 7110\n",
            "Total Draws: 0\n",
            "Total Losses: 1178\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 70, Agent 1 win %: 0.7\n",
            "Agent 2 wins: 24, Agent 2 win %: 0.24\n",
            "Ties: 6, Tie %: 0.06\n",
            "\n",
            "Iterations: 1000, Learn Rate: 0.1, Decay Rate: 0.8, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 737\n",
            "Total Draws: 0\n",
            "Total Losses: 143\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 69, Agent 1 win %: 0.69\n",
            "Agent 2 wins: 22, Agent 2 win %: 0.22\n",
            "Ties: 9, Tie %: 0.09\n",
            "\n",
            "Iterations: 2000, Learn Rate: 0.1, Decay Rate: 0.8, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 1436\n",
            "Total Draws: 0\n",
            "Total Losses: 138\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 71, Agent 1 win %: 0.71\n",
            "Agent 2 wins: 14, Agent 2 win %: 0.14\n",
            "Ties: 15, Tie %: 0.15\n",
            "\n",
            "Iterations: 5000, Learn Rate: 0.1, Decay Rate: 0.8, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 3953\n",
            "Total Draws: 0\n",
            "Total Losses: 321\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 65, Agent 1 win %: 0.65\n",
            "Agent 2 wins: 19, Agent 2 win %: 0.19\n",
            "Ties: 16, Tie %: 0.16\n",
            "\n",
            "Iterations: 10000, Learn Rate: 0.1, Decay Rate: 0.8, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 6620\n",
            "Total Draws: 0\n",
            "Total Losses: 1112\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 69, Agent 1 win %: 0.69\n",
            "Agent 2 wins: 22, Agent 2 win %: 0.22\n",
            "Ties: 9, Tie %: 0.09\n",
            "\n",
            "Iterations: 15000, Learn Rate: 0.1, Decay Rate: 0.8, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 10806\n",
            "Total Draws: 0\n",
            "Total Losses: 1742\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 46, Agent 1 win %: 0.46\n",
            "Agent 2 wins: 41, Agent 2 win %: 0.41\n",
            "Ties: 13, Tie %: 0.13\n",
            "\n",
            "Iterations: 20000, Learn Rate: 0.1, Decay Rate: 0.8, Explore-Exploit Threshold: 0.2\n",
            "=== Training Results ===\n",
            "Total Wins: 15145\n",
            "Total Draws: 0\n",
            "Total Losses: 2001\n",
            "=======================\n",
            "Results:\n",
            "Agent 1 wins: 64, Agent 1 win %: 0.64\n",
            "Agent 2 wins: 29, Agent 2 win %: 0.29\n",
            "Ties: 7, Tie %: 0.07\n",
            "\n"
          ]
        }
      ]
    }
  ]
}